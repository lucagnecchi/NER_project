{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucagnecchi/NER_project/blob/main/Copy_of_MyProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "html_code = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        /* Il tuo stile CSS qui */\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"banner\" style=\"color: white; background: linear-gradient(90deg, rgba(131,58,180,1) 0%, rgba(253,29,29,1) 50%, rgba(252,176,69,1) 100%);\">\n",
        "        <div class=\"text\" style=\"padding: 50px 30px 50px 30px; text-align: center;\">\n",
        "            <span style=\"font-size: 24pt; font-family: 'courier new', courier, monospace;\"><strong>Data Visualization</strong> and <strong>Text Mining</strong> project</span>\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "HTML(html_code)\n"
      ],
      "metadata": {
        "id": "XvU9aZupAaZx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "x0znFLRNCARb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Anatomical entities* are central to much of biomedical scientific discourse, the **AnEM corpus** is a dataset that can be used for the detection of *anatomical entities* in the text. The **AnEM corpus** was created by the *NaCTeM* (National Center for Text Mining). The corpus contains over 90000 words from 500 documents selected randomly from abstracts and full-text papers that are representative of the biomedical scientific literature.\n",
        "The **goal** of this project is to perform Named entity extraction on the corpus by using different machine learning architectures. The models will try to associate each word of the corpus with the correct tag."
      ],
      "metadata": {
        "id": "K4vDFB4JCDE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File format\n",
        "\n",
        "The corpus is stored in IOB2 format, a common tagging format for performing the *Name Entity Recognition* task. This is a standard format promoted by the **CoNLL (Conference on Computational Natural\n",
        "Language Learning)**. The *O* tag means that the token belongs to no chunks while the *B* tag is used at the beginning of every chunk and the *I* prefix indicates that a token is inside a chunk. In the table below are displayed all the labels used in this corpus."
      ],
      "metadata": {
        "id": "0SGfZ1ijvJIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th>Labels</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>Anatomical_system</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Cell</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Cellular_component</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Developing_anatomical_structure</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Immaterial_anatomical_entity</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Multi-tissue_structure</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Organ</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Organism_subdivision</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Organism_substance</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Pathological_formation</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Tissue</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "NL27597SBXc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries"
      ],
      "metadata": {
        "id": "0aZH5bSaCKh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install tensorflow_addons\n",
        "!pip install dash\n",
        "!pip install dash_bootstrap_components\n",
        "!pip install dash_core_components\n",
        "!pip install dash_html_components"
      ],
      "metadata": {
        "id": "R_YtxvQWn4L3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import urllib\n",
        "import sklearn\n",
        "import logging\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from seqeval.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import re\n",
        "import io\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from math import nan\n",
        "import spacy\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from spacy.tokens import Doc, Span\n",
        "from spacy import displacy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "#from tensorflow_addons.layers import CRF # Removed due to import error\n",
        "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import save_model, load_model\n",
        "from plotly.subplots import make_subplots\n",
        "import dash\n",
        "import dash_bootstrap_components as dbc\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output"
      ],
      "metadata": {
        "id": "vl8cURyQllhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ],
      "metadata": {
        "id": "8KfqnIRZWZAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/main/AnEM/data/AnEM.train\"\n",
        "test = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/main/AnEM/data/AnEM.test\"\n",
        "df_train = pd.read_csv(train, sep='\\t', header = None)\n",
        "df_test = pd.read_csv(test, sep='\\t', header = None)\n",
        "df = pd.concat((df_train, df_test))\n",
        "df.columns = [\"word\",\"start\",\"end\",\"ner_tag\"]\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "TbUqK7CvNxvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the first 10 rows of the database, we have that the dataset has 4 columns:\n",
        "- word: single word of the corpus\n",
        "- start: begin character position\n",
        "- end: end character position\n",
        "- ner_tag: classification label"
      ],
      "metadata": {
        "id": "a_D3ZTWvCm1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.where(df[\"ner_tag\"]!=\"O\").dropna()"
      ],
      "metadata": {
        "id": "NEoSJ8nCBN6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df=Counter(df[\"ner_tag\"])\n",
        "df[\"ner_tag_cat\"]=df[\"ner_tag\"].map(cat_df)"
      ],
      "metadata": {
        "id": "ob3mjPQI3nlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for missing values."
      ],
      "metadata": {
        "id": "51YRuqk1Mal6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "5ZyyMepJMxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilities functions for IOB"
      ],
      "metadata": {
        "id": "mfgbSyFkWsxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are dealing with IOB format, we have to define some functions in order to manipulate this type of file."
      ],
      "metadata": {
        "id": "qqcUedy2gnEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONLL_URL_ROOT = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/main/AnEM/data/\"\n",
        "\n",
        "def open_read_from_url(url):\n",
        "    \"\"\"\n",
        "    Take in input an url to a .txt file and return the list of its rows\n",
        "    \"\"\"\n",
        "    print(f\"Read file from {url}\")\n",
        "    file = urllib.request.urlopen(url)\n",
        "    lines = []\n",
        "    for line in file:\n",
        "        lines.append(line.decode(\"utf-8\"))\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_raw_conll(url_root, dir_path, filename):\n",
        "    \"\"\"Read a file which contains a conll03 dataset\"\"\"\n",
        "    lines = []\n",
        "    path = os.path.join(dir_path, filename)\n",
        "    full_url = url_root + filename\n",
        "    if os.path.isfile(path):\n",
        "        # read from file\n",
        "        print(f'Reading file {path}')\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "    else:\n",
        "        lines = open_read_from_url(full_url)\n",
        "    return lines[:]\n",
        "\n",
        "def load_conll_data(filename, url_root=CONLL_URL_ROOT, dir_path='',\n",
        "                    only_tokens=False):\n",
        "    \"\"\"\n",
        "    Take an url to the raw .txt files that you can find the repo linked above,\n",
        "    load data and save it into a list of tuples data structure.\n",
        "\n",
        "    Those files structure data with a word in each line with word, POS,\n",
        "    syntactic tag and entity tag separated by a whitespace. Sentences are\n",
        "    separated by an empty line.\n",
        "    \"\"\"\n",
        "    lines = read_raw_conll(url_root, dir_path, filename)\n",
        "    X = []\n",
        "    Y = []\n",
        "    sentence = []\n",
        "    labels = []\n",
        "    output_labels=set()\n",
        "    for line in lines:\n",
        "        if line == \"\\n\":\n",
        "            if(len(sentence) != len(labels)):\n",
        "                print(f\"Error: we have {len(sentence)} words but {len(labels)} labels\")\n",
        "            if sentence and is_real_sentence(only_tokens, sentence):\n",
        "                X.append(sentence)\n",
        "                Y.append(labels)\n",
        "            sentence = []\n",
        "            labels = []\n",
        "        else:\n",
        "            features = line.split()\n",
        "            tag = features.pop()\n",
        "            labels.append(tag)\n",
        "            output_labels.add(tag)\n",
        "            if only_tokens:\n",
        "                sentence.append(features.pop(0))\n",
        "            else:\n",
        "                sentence.append(tuple(features))\n",
        "\n",
        "    print(f\"Read {len(X)} sentences\")\n",
        "    if(len(X) != len(Y)):\n",
        "        print(\"ERROR in reading data.\")\n",
        "    return X, Y, output_labels\n",
        "\n",
        "def is_real_sentence(only_token, sentence):\n",
        "    \"\"\"Chek if a sentence is a real sentence or a document separator\"\"\"\n",
        "    first_word = \"\"\n",
        "    if only_token:\n",
        "        first_word = sentence[0]\n",
        "    else:\n",
        "        first_word = sentence[0][0]\n",
        "\n",
        "    if '---------------------' in first_word or first_word == '-DOCSTART-':\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "metadata": {
        "id": "NPhjSJPrOw1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/main/AnEM/data/AnEM.train\"\n",
        "test = \"https://raw.githubusercontent.com/nluninja/nlp_datasets/main/AnEM/data/AnEM.test\"\n",
        "train_csv = pd.read_csv(train, sep='\\t', header = None, quoting=3, skip_blank_lines = False)\n",
        "train_csv.columns = [\"word\",\"start\",\"end\",\"ner_tag\"]\n",
        "train_csv = train_csv[[\"word\", \"ner_tag\"]]\n",
        "test_csv= pd.read_csv(test, sep='\\t', header = None, quoting=3, skip_blank_lines = False)\n",
        "test_csv.columns = [\"word\",\"start\",\"end\",\"ner_tag\"]\n",
        "test_csv = test_csv[[\"word\", \"ner_tag\"]]\n",
        "df = pd.concat((train_csv, test_csv))\n",
        "df.columns = [\"word\",\"ner_tag\"]\n",
        "\n",
        "train_csv.to_csv('train.txt', sep=' ', header=False, index=False, na_rep='', lineterminator='\\n')\n",
        "test_csv.to_csv('test.txt', sep=' ', header=False, index=False, na_rep='', lineterminator='\\n')\n",
        "\n",
        "data_dir = os.path.join('data', 'conll03')\n",
        "raw_train, ner_train, output_labels = load_conll_data('AnEM.train', dir_path=data_dir, only_tokens=True)\n",
        "\n",
        "raw_test, ner_test, _ = load_conll_data('AnEM.test', dir_path=data_dir, only_tokens=True)\n"
      ],
      "metadata": {
        "id": "cC2MoTwmHIDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *train_csv* and *test_csv* contain tables with two columns representing the word and their frequency according to the train and test set\n",
        "- *raw_train* contains all the sentences in the train set (the sentences are separated with a blank line)\n",
        "- *ner_train* contains all the tags associated to the words in the train set\n",
        "- *output_labels* contains all the distinct tags in the dataset"
      ],
      "metadata": {
        "id": "ud8_uT4oLNo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "\n",
        "app.layout = dbc.Container([\n",
        "    dbc.Row(\n",
        "        dbc.Col(html.H1(\"AnEM Dashboard\", className='text-center text-primary mb-4'), width=12)\n",
        "    ),\n",
        "    dbc.Row([\n",
        "        dcc.Dropdown(\n",
        "            id='my-dpdn',\n",
        "            multi=False,\n",
        "            value='tag distribution',\n",
        "            options=['tag distribution', 'sentences distribution', 'pie chart']\n",
        "        ),\n",
        "        dcc.Graph(id='line-fig', figure={})\n",
        "    ], justify='start'),\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('line-fig', 'figure'),\n",
        "    Input('my-dpdn', 'value')\n",
        ")\n",
        "def update_graph(graph_selected):\n",
        "    if graph_selected == \"tag distribution\":\n",
        "        df_hist = df.where(df['ner_tag'] != 'O').dropna()\n",
        "        figln = px.histogram(df_hist, x='ner_tag', title='Tag Distribution')\n",
        "\n",
        "    elif graph_selected == \"sentences distribution\":\n",
        "        df_freq = pd.DataFrame(pd.concat((pd.Series(raw_train), pd.Series(raw_test))))\n",
        "        df_freq.columns = ['sentence']\n",
        "        length_for_samples = [len(j) for j in df_freq['sentence']]\n",
        "        data = pd.DataFrame(length_for_samples)\n",
        "        data.columns = ['length_sentence']\n",
        "        figln = px.histogram(data, x='length_sentence', title='Sentences Distribution')\n",
        "\n",
        "    elif graph_selected == 'pie chart':\n",
        "        flatten_ner = dict(pd.Series([j for j in df['ner_tag'] if j != 'O']).value_counts())\n",
        "        label_union = [item.split(sep='-')[1] for item in flatten_ner.keys()]\n",
        "        label_dict = {}\n",
        "        for item in label_union:\n",
        "          label_dict[item] = 0\n",
        "        for k,v in zip(flatten_ner.keys(), flatten_ner.values()):\n",
        "          if k.split(sep = \"-\")[1] in label_dict:\n",
        "            label_dict[k.split(sep = \"-\")[1]] = label_dict[k.split(sep = \"-\")[1]] + v\n",
        "          else:\n",
        "            label_dict[k.split(sep = \"-\")[1]] = v\n",
        "        labels = list(label_dict.keys())\n",
        "        values = list(label_dict.values())\n",
        "        figln = px.pie(names=labels, values=values, title='Pie Chart')\n",
        "\n",
        "    return figln\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "6MRYkvoUsTEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blanks = []\n",
        "\n",
        "# (index,label,review text)\n",
        "for index,label,review in train_csv.itertuples():  # iterate over the DataFrame\n",
        "    if type(review)==str:            # avoid NaN values\n",
        "        if review.isspace():         # test 'review' for whitespace\n",
        "            blanks.append(index)     # add matching index numbers to the list\n",
        "\n",
        "print(blanks)\n",
        "blanks = []\n",
        "\n",
        "# (index,label,review text)\n",
        "for index,label,review in test_csv.itertuples():  # iterate over the DataFrame\n",
        "    if type(review)==str:            # avoid NaN values\n",
        "        if review.isspace():         # test 'review' for whitespace\n",
        "            blanks.append(index)     # add matching index numbers to the list\n",
        "\n",
        "print(blanks)"
      ],
      "metadata": {
        "id": "4b3DwJ7dQjkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**en_core_web_lg** is a Spacy pipeline with different components for performing NLP. This model was trained on written texts such as blogs, news and comments. The operations performable by this model includes Part Of Speech tagging, Named entity recognition, Dependency parsing, Word embeddings. In the case of word embedding, this model associate each word to a vector of lenght 300."
      ],
      "metadata": {
        "id": "KCA5ytVX0D6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg', disable=['parser', 'tagger', 'ner'])"
      ],
      "metadata": {
        "id": "Hpd-rARTSU9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can visualise the entities according to each sentence."
      ],
      "metadata": {
        "id": "gfHHd1wTfgKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize named entities using spaCy's displacy\n",
        "def visualize_entities(words, tags):\n",
        "    # Create a spaCy Doc object for the sentence\n",
        "    doc = Doc(nlp.vocab, words=words)\n",
        "\n",
        "    # Extract entities for the sentence\n",
        "    entities = [(i, i + 1, tag.split('-')[-1]) for i, tag in enumerate(tags) if '-' in tag]\n",
        "    selected_entities = [(start, end, label) for start, end, label in entities\n",
        "                         if 0 <= start < len(words) and 0 <= end <= len(words)]\n",
        "\n",
        "    doc.ents = [Span(doc, start, end, label=label) for start, end, label in selected_entities]\n",
        "\n",
        "    # Clear the output before displaying the new visualization\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        # Create a list of colors for each unique entity type\n",
        "        unique_entities = set(entity[2] for entity in selected_entities)\n",
        "        entity_colors = {entity: f'#{hash(entity) & 0xFFFFFF:06x}' for entity in unique_entities}\n",
        "\n",
        "        # Set up options for displacy.render\n",
        "        options = {'ents': list(unique_entities), 'colors': entity_colors}\n",
        "\n",
        "        # Visualize named entities using spaCy's displacy\n",
        "        displacy.render(doc, style='ent', jupyter=True, options=options)\n",
        "\n",
        "        # Display words with their corresponding tags below the sentence\n",
        "        for word, tag in zip(words, tags):\n",
        "            print(f\"{word}: {tag}\")\n",
        "\n",
        "# Create a BoundedIntText widget\n",
        "bounded_int_text = widgets.BoundedIntText(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=len(raw_train)-1,\n",
        "    step=1,\n",
        "    description='Sentence:',\n",
        "    disabled=False)\n",
        "\n",
        "# Create an output widget\n",
        "output = widgets.Output()\n",
        "\n",
        "# Define a function to update the visualization based on the widget value\n",
        "def update_visualization(change):\n",
        "    value = change['new']\n",
        "    visualize_entities(raw_train[value], ner_train[value])\n",
        "\n",
        "# Attach the update_visualization function to the widget's value change\n",
        "bounded_int_text.observe(update_visualization, 'value')\n",
        "\n",
        "# Display the container with the BoundedIntText widget and the output area\n",
        "display(widgets.VBox([bounded_int_text, output]))\n",
        "\n",
        "# Initial visualization\n",
        "update_visualization({'new': bounded_int_text.value})"
      ],
      "metadata": {
        "id": "md5rBgDqeGVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords = [\"fig\"]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    processed_tokens = [token.lemma_ for token in doc if not (token.is_stop or token.is_punct or len(token.text) == 1 or token.text in custom_stopwords)]\n",
        "    return ' '.join(processed_tokens)"
      ],
      "metadata": {
        "id": "larHAE9nRhRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = train_csv.dropna()"
      ],
      "metadata": {
        "id": "_BcKUw6vTwTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_csv['word'] = train_csv['word'].apply(preprocess_text)\n",
        "# Filter out rows with empty 'Processed' column\n",
        "train_csv = train_csv[train_csv['word'].str.strip() != '']\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(train_csv['word'])\n",
        "\n",
        "# Get the top 100 words\n",
        "top_words = word_freq.most_common(100)\n",
        "\n",
        "# Create a DataFrame\n",
        "word_df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
        "word_df"
      ],
      "metadata": {
        "id": "Ixq22L7VRiSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_dict = dict(zip(word_df['Word'], word_df['Frequency']))\n",
        "\n",
        "png_url = \"https://i.ibb.co/NrTyKCn/127880568-icona-medica-dell-ospedale-della-croce-nera-isolato-su-priorit-bassa-bianca-primo-soccorso.jpg\"\n",
        "response = requests.get(png_url)\n",
        "img = Image.open(io.BytesIO(response.content))\n",
        "mask_map = np.array(img)\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      contour_width=1,  # Adjust this value to make the contours visible\n",
        "                      contour_color='white', mask=mask_map)\n",
        "wordcloud.generate_from_frequencies(word_freq_dict)\n",
        "\n",
        "\n",
        "# Display the word cloud using Matplotlib\n",
        "fig = plt.figure(figsize=(8,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off the axis\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HjCjYOjHWZuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation set"
      ],
      "metadata": {
        "id": "VWyH-1HnX8h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create a *validation set* we select randomly the 20% of the *training set*."
      ],
      "metadata": {
        "id": "MLLCNZda3mMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of indices to select (20% of the length of the original list)\n",
        "num_indices_to_select = int(0.20 * len(raw_train))\n",
        "\n",
        "# Use random.sample to select random indices without replacement\n",
        "random.seed(42)\n",
        "validation_indices = random.sample(range(len(raw_train)), num_indices_to_select)"
      ],
      "metadata": {
        "id": "Gj1l-9Lu3qZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_indices_to_select"
      ],
      "metadata": {
        "id": "vUdy0WNM5Y-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_valid = [raw_train[i] for i in validation_indices]\n",
        "ner_valid = [ner_train[i] for i in validation_indices]\n",
        "\n",
        "raw_train = [raw_train[i] for i in range(len(raw_train)) if i not in validation_indices]\n",
        "ner_train = [ner_train[i] for i in range(len(ner_train)) if i not in validation_indices]\n",
        "\n",
        "print(\"Length of validation set:\", len(raw_valid))\n",
        "print(\"Length of validation NER set:\", len(ner_valid))\n",
        "print(\"Length of updated training set:\", len(raw_train))\n",
        "print(\"Length of updated training NER set:\", len(ner_train))"
      ],
      "metadata": {
        "id": "WZ2EfyMu388o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are converting words into tokens with the *keras* built-in function *Tokenizer()*, we fit this object on our corpus\n",
        "- With the method *texts_to_sequences()* we can convert each set into vectors, we store the results in the variables *train_sequences*, *test_sequences* and *valid_sequences*\n",
        "- In *tag2idx* we are associating each tag to the corresponding *output label*\n",
        "- In *idxtag* we have the opposite, we have as keys the *output labels* and as values we have the tag\n",
        "- With list comprehension we are creating other lists that contains, for each sentences, the associated output label"
      ],
      "metadata": {
        "id": "zTVLSSJla9qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode sequences of words\n",
        "token_tokenizer = Tokenizer()    # Automatically lowers tokens\n",
        "token_tokenizer.fit_on_texts(raw_train + raw_test + raw_valid)\n",
        "train_sequences = token_tokenizer.texts_to_sequences(raw_train)\n",
        "test_sequences = token_tokenizer.texts_to_sequences(raw_test)\n",
        "valid_sequences = token_tokenizer.texts_to_sequences(raw_valid)\n",
        "\n",
        "tag2idx = { tag: idx for idx, tag in enumerate(output_labels) }\n",
        "idx2tag = { idx: tag for tag, idx in tag2idx.items() }\n",
        "ner_train_sequences = [[tag2idx[tag] for tag in sentence] for sentence in ner_train]\n",
        "ner_test_sequences  = [[tag2idx[tag] for tag in sentence] for sentence in ner_test ]\n",
        "ner_valid_sequences = [[tag2idx[tag] for tag in sentence] for sentence in ner_valid]"
      ],
      "metadata": {
        "id": "pODd7WQVxZwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the *ner_train_sequences* we have a lot of 4 because it corresponds to the O tag."
      ],
      "metadata": {
        "id": "2XUJQybjkJ2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tag2idx)\n",
        "print(idx2tag)\n",
        "#we consider the first sentence of the train set\n",
        "print(raw_train[0])\n",
        "#we consider the associated labels\n",
        "print(ner_train_sequences)"
      ],
      "metadata": {
        "id": "lRsjyYRdfwN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_test[0])\n",
        "print(test_sequences[0])\n",
        "for i in test_sequences[0]:\n",
        "    print(f'{i} : {token_tokenizer.index_word[i]}')"
      ],
      "metadata": {
        "id": "hpjDZqyLzpqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(token_tokenizer.word_counts)\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "id": "zhSHCEz5zq55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_train[0])\n",
        "print(ner_train_sequences[0])\n",
        "for i in ner_train_sequences[0]:\n",
        "    print(f'{i} : {idx2tag[i]}')"
      ],
      "metadata": {
        "id": "7k86DC3xzwCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are identyfing the lenght of the sequences, we are saving the maximum lenght and the percentiles distribution."
      ],
      "metadata": {
        "id": "guHrlATPoWtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_len = np.array([len(s) for s in train_sequences])\n",
        "longest_sequence = sequence_len.max()\n",
        "print(f'Longest sequence: {longest_sequence}')\n",
        "\n",
        "print([(str(p) + '%', np.percentile(sequence_len, p)) for p in range(75,101, 5)])"
      ],
      "metadata": {
        "id": "dcMOyjubz2Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding**"
      ],
      "metadata": {
        "id": "RG7Znyb-L80f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are setting all the inputs with the same dimension, each sentence is a vector of lenght 334. In the sentences with lenght less than 334 we fill the empty cells with 0."
      ],
      "metadata": {
        "id": "5GU8TugzpYuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = longest_sequence\n",
        "X_train = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n",
        "X_valid = pad_sequences(valid_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n",
        "\n",
        "Y_train = pad_sequences(ner_train_sequences, maxlen=max_sequence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
        "Y_test = pad_sequences(ner_test_sequences, maxlen=max_sequence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
        "Y_valid = pad_sequences(ner_valid_sequences, maxlen=max_sequence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
        "\n",
        "#one-hot encoding\n",
        "Y_train = to_categorical(Y_train, num_classes=len(output_labels), dtype='int32')\n",
        "Y_test = to_categorical(Y_test, num_classes=len(output_labels), dtype='int32')\n",
        "Y_valid = to_categorical(Y_valid, num_classes=len(output_labels), dtype='int32')"
      ],
      "metadata": {
        "id": "78m926Q56rpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "print(Y_train[0])"
      ],
      "metadata": {
        "id": "l7HTJ7Ea6v9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_tokenizer.index_word[0] = '_PAD_'"
      ],
      "metadata": {
        "id": "1sTAFLAp60kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are converting *X_train* and *Y_train* into numpy arrays."
      ],
      "metadata": {
        "id": "beQJ5iYSqP4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test)\n",
        "X_valid = np.array(X_valid)\n",
        "Y_valid = np.array(Y_valid)"
      ],
      "metadata": {
        "id": "hNuGRzfh623q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "metadata": {
        "id": "n3uVaoim66cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed forward"
      ],
      "metadata": {
        "id": "9XdRhlVGK4UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **feedforward neural network** is an architecture in which the information flows from one layer to next one, from the input layer, through the hidden layer and to the output layer.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://afit-r.github.io/public/images/analytics/deep_learning/fig18_1.png\" />\n",
        "</center>\n",
        "\n",
        "Our model has 3 layers:\n",
        "\n",
        "- **Embedding layer (input layer)**: in this layer we are performing a *word embedding* procedure on our input. We are setting the *input dimension* that corresponds to the dimension of the vocabulary +1. Keep in mind that the vocabulary is the set of unique words in the corpus, we are adding one unit to it because we want to consider also tokens not present in the vocabulary. The *input lenght* parameter corresponds to the dimension of the longest sequence in the corpus. Recall that the aim of the *word embedding* is to represent each word with a vector. With the *embedding_dim* parameter we can specify the dimension of the vectors that represent each word. The larger the *embedding_dim parameter*, the greater the complexity of the model.\n",
        "- **Dense layer (hidden layer)**: this is a fully connected layer with 50 nodes and the *ReLu* activation function.\n",
        "- **Dense layer (output layer)**: this is the last layer of the model. Since we want to perform a classification with 23 labels, the output dimension is 23 and we use a *SoftMax* activation function.\n",
        "\n",
        "With the method *.compile()* of the class *Sequential()* we can define the essential features of the model before its training:\n",
        "- **Optimizer**: we defined *adam* optimizer that defines how the model's weights are updated. It is a variation of the *SDG* method that adjusts adaptively the learning rate in order to speeding up the convergence of the minimum.\n",
        "- **Loss function**: since we are dealing with a multi label classification we consider the *categorical_crossentropy* function.\n",
        "- **Metrics**: we are using as metrics to measure the performance of the model the *accuracy*. Recall that the *accuracy* represents the ratio of correct predictions to the total number of predictions.\n"
      ],
      "metadata": {
        "id": "tzNUdrghMDTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "YIvelbeCaJjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 57\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocabulary_size+1,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=longest_sequence,\n",
        "                           ))\n",
        "model.add(layers.Dense(50, activation='relu'))\n",
        "model.add(layers.Dense(23, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              #multiclass classification\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zAgoGWptMItK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting"
      ],
      "metadata": {
        "id": "WjrW3eJkaMc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally fit the model by using the *.fit()* method. We are defining 20 *epochs*, an *epoch* is completed when a model is fitted to the whole train set. This means that we are fitting the model on the train set 20 times. We are also specifying the *batch_size* that is the dimension of the subset of the train set that we are processing at each iteration."
      ],
      "metadata": {
        "id": "qCc0eeK6MLEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "                    epochs=20,\n",
        "                    #I am showing the loading bar\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    batch_size=32)\n",
        "loss, accuracy = model.evaluate(X_train, Y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, Y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "metadata": {
        "id": "IeGx3-8TMLx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to define some useful functions in order to plot the: **validation accuracy**, **validation loss**, **training accuracy** and **training loss**."
      ],
      "metadata": {
        "id": "XEcaC305MP-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function *plot_history_csv()* plots the graphs of accuracy and loss trends starting from a *.csv* containing the accuracy and loss values. We decided to store this values in a separated file in order to have a backup in the case in which some errors occur in the colab notebooks."
      ],
      "metadata": {
        "id": "oYBvEDPdMS74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history_csv(history_df):\n",
        "\n",
        "    # we are creating a dictionary that we will convert into a dataframe\n",
        "    data = {\n",
        "        'Epoch': list(range(1, len(history_df['accuracy']) + 1)),\n",
        "        'Training Accuracy': history_df['accuracy'],\n",
        "        'Validation Accuracy': history_df['val_accuracy'],\n",
        "        'Training Loss': history_df['loss'],\n",
        "        'Validation Loss': history_df['val_loss']\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # container\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=['Training and Validation Loss', 'Training and Validation Accuracy'])\n",
        "\n",
        "    # loss graph\n",
        "    fig.add_trace(go.Scatter(x=df['Epoch'], y=df['Training Loss'], mode='lines+markers', name='Training Loss'), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=df['Epoch'], y=df['Validation Loss'], mode='lines+markers', name='Validation Loss'), row=1, col=1)\n",
        "\n",
        "    # accuracy graph\n",
        "    fig.add_trace(go.Scatter(x=df['Epoch'], y=df['Training Accuracy'], mode='lines+markers', name='Training Accuracy'), row=1, col=2)\n",
        "    fig.add_trace(go.Scatter(x=df['Epoch'], y=df['Validation Accuracy'], mode='lines+markers', name='Validation Accuracy'), row=1, col=2)\n",
        "\n",
        "\n",
        "    fig.update_yaxes(title_text='Loss', row=1, col=1)\n",
        "    fig.update_yaxes(title_text='Accuracy', row=1, col=2)\n",
        "\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "dVoExCgyMWxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to store the results in a *.csv* file we can run the following code after the training of the model."
      ],
      "metadata": {
        "id": "TI4QOm24MY2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_values = {\n",
        "    'loss': history.history['loss'],\n",
        "    'accuracy': history.history['accuracy'],\n",
        "    'val_loss': history.history['val_loss'],\n",
        "    'val_accuracy': history.history['val_accuracy'],\n",
        "}\n",
        "\n",
        "history_df = pd.DataFrame(history_values)\n",
        "\n",
        "# we store the history in the following file\n",
        "history_df.to_csv('history_ff.csv', index=False)"
      ],
      "metadata": {
        "id": "6zUgiCK1MZR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also save the model in the following way:"
      ],
      "metadata": {
        "id": "0KNIy5GBMhA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('Feed_Forward.h5')"
      ],
      "metadata": {
        "id": "DraGHsjYMhyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphical representation of the performance"
      ],
      "metadata": {
        "id": "7_TsuzXFaQt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.read_csv('history_ff.csv')\n",
        "plot_history_csv(history_df)"
      ],
      "metadata": {
        "id": "-G1zBf0kMyLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification report"
      ],
      "metadata": {
        "id": "1GDc5wxIaaFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_encode_to_literal_labels(y_true, y_pred, idx2tag):\n",
        "    '''Transform sequences of encoded labels in sequences of string labels'''\n",
        "    let_y_true = list()\n",
        "    let_y_pred = list()\n",
        "    for sent_idx in range(len(y_true)):\n",
        "        let_sent_true = []\n",
        "        let_sent_pred = []\n",
        "        for token_idx in range(len(y_true[sent_idx])):\n",
        "            let_sent_true.append(idx2tag[y_true[sent_idx][token_idx]])\n",
        "            let_sent_pred.append(idx2tag[y_pred[sent_idx][token_idx]])\n",
        "        let_y_true.append(let_sent_true)\n",
        "        let_y_pred.append(let_sent_pred)\n",
        "\n",
        "    return let_y_true, let_y_pred\n",
        "\n",
        "def remove_seq_padding(X, y_true, y_pred, pad=0):\n",
        "    \"\"\"Remove padding predictions from list of sequences\"\"\"\n",
        "    new_true = []\n",
        "    new_pred = []\n",
        "    for sent_idx in range(len(X)):\n",
        "        true_sent = []\n",
        "        pred_sent = []\n",
        "        for tok_idx in range(len(X[sent_idx])):\n",
        "            if X[sent_idx][tok_idx] != pad:\n",
        "                true_sent.append(y_true[sent_idx][tok_idx])\n",
        "                pred_sent.append(y_pred[sent_idx][tok_idx])\n",
        "        new_true.append(true_sent)\n",
        "        new_pred.append(pred_sent)\n",
        "    return np.array(new_true), np.array(new_pred)"
      ],
      "metadata": {
        "id": "_S1BZR5ENOrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [('Training Set', X_train, Y_train), ('Test Set', X_test, Y_test), ('Validation Set', X_valid, Y_valid)]\n",
        "batch_size = 32\n",
        "for title, X, Y in datasets:\n",
        "    Y_pred = model.predict(X, batch_size=batch_size)\n",
        "    Y_pred = np.array(np.argmax(Y_pred, axis=-1))\n",
        "    Y = np.array(np.argmax(Y, axis=-1))\n",
        "    Y, Y_pred = remove_seq_padding(X, Y, Y_pred)\n",
        "    let_y_true, let_y_pred = from_encode_to_literal_labels(Y, Y_pred, idx2tag)\n",
        "\n",
        "    print(title)\n",
        "    print(classification_report(let_y_true, let_y_pred, digits=3))\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "Nrd7526SM75u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to understand the classification report we must understand the concepts of **precision**, **recall**, **f1-score**.\n",
        "- $\\text{precision}: \\frac{TP}{TP+FP}$\n",
        "-$\\text{recall}: \\frac{TP}{TP+FN}$\n",
        "-$\\text{f1-score}$ harmonic mean of precision and recall\n",
        "\n",
        "We have three indicators to measure the performance of the model:\n",
        "- micro avg: calculates precision, recall, and F1-score by aggregating results from individual examples\n",
        "- macro avg: calculates the average of metrics independently for each class and then takes the unweighted mean (withot considering class unbalance)\n",
        "- weighted avg: calculates the weighted average of metrics, where the weight is the proportion of examples in each class relative to the total."
      ],
      "metadata": {
        "id": "9GktoFQz5OME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM"
      ],
      "metadata": {
        "id": "4B9dmZMLWWz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are implementing the **BiLSTM** model. In order to understand this model it is necessary to review some concepts about **RNN** and **LSTM** cell.\n",
        "- **RNN (Recurrent Neural Netwok)**: architecture used when we are dealing with sequential data where a single data unit is influenced by the previous ones. In this models the output of a node becomes the input of the node itself. \\\\\n",
        "The problem with **RNN** is that, especially when they are dealing with large dimension input, they forget the first inputs.\n",
        "- **LSTM (Long Short Term Memory)**: this kind of architecture tries to address the issues of the **RNN** by introducing the *conveyor belt* a variable that memorises information about the previous layers of the architecture."
      ],
      "metadata": {
        "id": "YH4U1N4HWiAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "  <img src=\"https://www.researchgate.net/profile/Sayed-Rafay-Bin-Shah-2/publication/353371672/figure/fig1/AS:1088315499524096@1636486043257/Structure-of-the-LSTM-cell.png\" width = 600 height = 450/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "MYPM5V7-cLko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the Figure we have that the **LSTM** cell has 3 gates:\n",
        "- Forget gate: we are applying a *sigmoid* activation function to the previous hidden state $h_{t-1}$ and the input $x_t$ in order to decide which information to keep. Then we are multypling this result with the previous cell state $C_{t-1}$.\n",
        "- Input gate: we are applying to the input and the previous hidden state the *tangent hyperbolic* function and the *sigmoid* function in order to obtain a *candidate value* $\\tilde{C}_t$. Subsequentially we are obtaining the cell state $C_t$ by combining the *candidate value* with the output of the forget gate.\n",
        "- Output gate: at the end of the day we are applying the *tangent hyperbolic* function to the cell state $C_t$ and we are obtaining the *hidden state* $h_t$ of the **LSTM** cell by combining it with the input and the previous cell state."
      ],
      "metadata": {
        "id": "6uR5kdzRctMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the **BiLSTM** architecture is composed by two **LSTMs** that take the input in the *forward* and *backward* direction both. In this way we are improving the context available to the algorithm by knowing the words that immediate follow and preced a word in a sentence.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-25_at_8.54.27_PM.png\" height = 300/>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "V_ChwkBGWWUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "UTgXSOPDaix1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we define a function to create the **BiLSTM** model:"
      ],
      "metadata": {
        "id": "UIr7dnnek6uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_BiLSTM(vocabulary_size, seq_len, n_classes, hidden_cells=200,\n",
        "                  embed_dim=100, drop=0.4, use_glove=False, glove_matrix=None):\n",
        "    \"\"\"Create a BiLSTM model using keras, given its parameters\"\"\"\n",
        "    model = Sequential()\n",
        "    if use_glove:\n",
        "        model.add(Embedding(vocabulary_size, embed_dim,\n",
        "                            weights=[glove_matrix], input_length=seq_len,\n",
        "                            mask_zero=True, trainable=True))\n",
        "    else:\n",
        "        model.add(Embedding(vocabulary_size, embed_dim, input_length=seq_len,\n",
        "                            mask_zero=True))\n",
        "    #dropout layer\n",
        "    model.add(Dropout(drop))\n",
        "\n",
        "    #bidirectional layer\n",
        "    model.add(Bidirectional(LSTM(hidden_cells, return_sequences=True,\n",
        "                                 dropout=drop)))\n",
        "    #output layer\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy',\n",
        "                           Precision(),\n",
        "                           Recall()])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TTfvBhXPlIZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding: GloVe"
      ],
      "metadata": {
        "id": "P8yLiGLAnjNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since machine learning models are not able to process text, we need to generate features from a text document. In order to have a mathematical representation of the words in a corpus we can rely on **vector space models**, we can represent each word in the corpus as a n-dimensional vector. \\\\\n",
        "With **GloVe**, each word is represented as a n-dimensional vector that caputures its semantic and synctactic difference. While **CBOW** tries to predict the current word based on the surrounding words and **Skip-gram** tries to predict the context words given a target word, **GloVe** tries to capture the relationship between words based on how frequently they appear together."
      ],
      "metadata": {
        "id": "wfpjfFdVL321"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe file\n",
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# Move the required file to the current directory\n",
        "!mv glove.6B.100d.txt /content"
      ],
      "metadata": {
        "id": "Sm4uRw2qnitA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run only once\n",
        "os.remove('/content/glove.6B.300d.txt')\n",
        "os.remove('/content/glove.6B.200d.txt')\n",
        "os.remove('/content/glove.6B.50d.txt')\n",
        "os.remove('/content/glove.6B.zip')"
      ],
      "metadata": {
        "id": "rL-wbTTAODfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embedding_matrix(path, word_index, embed_dim):\n",
        "    \"\"\"Load Glove embeddings.\n",
        "\n",
        "    More info here:\n",
        "    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embed_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "JvcJS6uapiUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "5EWTovaXh3HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GLOVE = True\n",
        "glove_matrix=None\n",
        "if USE_GLOVE:\n",
        "    glove_embedding_path = os.path.join('/content/', 'glove.6B.100d.txt')\n",
        "    embedding_dim = 100\n",
        "    glove_matrix = load_glove_embedding_matrix(glove_embedding_path, token_tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "mpwKKulGPZwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check how many words are missing in *GloVe* pre-trained embeddings that we have downloaded before:"
      ],
      "metadata": {
        "id": "ZcI5tXZOaVkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_missing_words(embedding_matrix):\n",
        "    # Find the indices of zero rows\n",
        "    zero_rows = np.all(embedding_matrix == 0, axis=1)\n",
        "\n",
        "    # Extract the words that have zero rows\n",
        "    missing_words = [word for word, is_zero in zip(tokenizer.word_index.keys(), zero_rows) if is_zero]\n",
        "\n",
        "    return missing_words\n",
        "\n",
        "# Check for missing words in the embedding matrix\n",
        "missing_words = check_for_missing_words(glove_emb_matrix)\n",
        "\n",
        "print(\"Words not found in pre-trained embeddings:\", missing_words)\n",
        "print(\"Total words missing:\", len(missing_words))"
      ],
      "metadata": {
        "id": "RAyqXsEkavhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = my_BiLSTM(vocabulary_size+1, max_sequence_len, len(output_labels),\n",
        "                                 use_glove=USE_GLOVE, glove_matrix=glove_matrix)\n",
        "\n",
        "best_model_file = os.path.join('models','lstm-conll-best-model.h5')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    best_model_file,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# early stopping if the difference in loss of two epochs is less than 0.01\n",
        "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=3, verbose=1, mode=\"auto\", restore_best_weights=True)"
      ],
      "metadata": {
        "id": "Pgr_ycYhP4WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting"
      ],
      "metadata": {
        "id": "t7Gtt6W_anvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "batch_size = 10\n",
        "history = model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          verbose=2,\n",
        "          callbacks=[checkpoint, early_stopping_callback],\n",
        "          validation_data=(X_valid, Y_valid)\n",
        "         )"
      ],
      "metadata": {
        "id": "9OwWQGejQHgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_memory_usage(batch_size, model):\n",
        "    \"\"\"Print memory usage of a model in MB given the batch size\"\"\"\n",
        "    mbytes = get_model_memory_usage(batch_size, model)\n",
        "    print(f'Model size: {mbytes} MB')"
      ],
      "metadata": {
        "id": "a3m3MrfGpRyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_memory_usage(batch_size, model)"
      ],
      "metadata": {
        "id": "zpKSU-BdRixi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_values = {\n",
        "    'loss': history.history['loss'],\n",
        "    'accuracy': history.history['accuracy'],\n",
        "    'val_loss': history.history['val_loss'],\n",
        "    'val_accuracy': history.history['val_accuracy'],\n",
        "}\n",
        "\n",
        "history_df = pd.DataFrame(history_values)\n",
        "\n",
        "# we store the history in the following file\n",
        "history_df.to_csv('history_BiLSTM.csv', index=False)"
      ],
      "metadata": {
        "id": "hw8fZm2ErMCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('BiLSTM.h5')"
      ],
      "metadata": {
        "id": "WkyhaOpNrVvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphical representation of the model"
      ],
      "metadata": {
        "id": "TzTYWNQQaqzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.read_csv('history_BiLSTM.csv')\n",
        "plot_history_csv(history_df)"
      ],
      "metadata": {
        "id": "Dbj963Ldrgvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification report"
      ],
      "metadata": {
        "id": "WU8nSz9rau0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [('Training Set', X_train, Y_train), ('Test Set', X_test, Y_test), ('Validation Set', X_valid, Y_valid)]\n",
        "\n",
        "for title, X, Y in datasets:\n",
        "    Y_pred = model.predict(X, batch_size=batch_size)\n",
        "    Y_pred = np.array(np.argmax(Y_pred, axis=-1))\n",
        "    Y = np.array(np.argmax(Y, axis=-1))\n",
        "    Y, Y_pred = remove_seq_padding(X, Y, Y_pred)\n",
        "    let_y_true, let_y_pred = from_encode_to_literal_labels(Y, Y_pred, idx2tag)\n",
        "\n",
        "    print(title)\n",
        "    print(classification_report(let_y_true, let_y_pred, digits=3))\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "q55pEHEFRusZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 5\n",
        "sentence = X_test[i]\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred = y_pred[i]\n",
        "y_true = np.argmax(Y_test, axis=-1)[i]\n",
        "for idx in range(len(sentence)):\n",
        "    print(f'{token_tokenizer.index_word[sentence[idx]]:15}  {idx2tag[y_true[idx]]:6} | {idx2tag[y_pred[idx]]}')\n"
      ],
      "metadata": {
        "id": "fph8UcaBUQ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "PQjVbSTsPFv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will implement the BERT model. It is a model created by Google that revolutioned the NLP, and we will use it for Named Entity Recognition. Its power is the fact that it captures bidirectional context information and it was pretrained. Actually, it uses the \"transfer learning\" to increase its performance, thank to a large amount of unlabelled data that were given to it.\n"
      ],
      "metadata": {
        "id": "4YFTnt_-UdeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to understand the BERT model we must review some concepts about: **encoders, decoders** and **transformers.**\n",
        "## Encoders and decoders\n",
        "The **encoder block** is composed with LSTM cells in which each cell updates its hidden state $h_t$ according to its input $x_t$ and the previous hidden states $h_{t-1}$. The output of the **encoder block** is passed as input for the **decoder block**. In this block the output at each time step is computed according to the output of the previous cell and the internal states ($h_{t}, C_{t}$).\n",
        "The problem of this approach is the <font color='red'>information bottleneck</font>: the output of the **encoder** must represent the meaning of the whole sequence passed as input and with long sequences this task is hard. With the **attention mechanism** the encoder passes all the hidden states to the **decoder** but, at every element of the **decoder**. The **decoder** looks at the set of hidden states that received, determines a score for each hidden state and does not consider the ones with lowest score. With the **attention mechanism** we have that each cell of the **decoder** takes as input a function of the previous hidden states and outputs of the **decoder**, and also the context vector that represent a sort of weighted sum of all **encoders** hidden states.\n",
        "<center>\n",
        "<img align = \"center\" src = \"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0aHodc667UfSyZj-UY8OQw.png\" width = 600 img>\n",
        "<center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mf_4u7V_qX0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Transformers\n",
        "The important novelty of this architecture is that, with respect to the previouse seq-to-seq models, **transformers** do not rely on Recurrent Neural Networks. First of all, since we do not rely on RNN, we introduce the **positional encoding** that gives to each word took as input a relative position. This architecture introduce also the **multi-head attention** mechanism in which the model uses multiple set of attention weights in paralles. This caveat allows the model to capture different types of relationships and dependencies between words.\n",
        "\n",
        "<center>\n",
        "<img src = \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" height = 500<img>\n",
        "<center>"
      ],
      "metadata": {
        "id": "1Nhm4s4y11QY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT** is a powerful language model that learns to understand words in context. It uses a bidirectional approach to consider both the past and future words in a sentence.\n",
        "The **BERT** model follows the *transfer learning* paradigm, it is pretrained on a large text corpus, enabling it to learn deep contextual representations."
      ],
      "metadata": {
        "id": "VcUFc1L6G9Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from simpletransformers.ner import NERModel, NERArgs"
      ],
      "metadata": {
        "id": "ToQssupNPdHY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the two following chunk we create the dataset as it's needed by the model: we are creating train and test dataset organized in tokeinzed words with the corresponding label and the number of the sentence. This last attribute will be create with a function that will increase the 'sentence_id' attribute at each dot. We know it's not the perfect way to do this task but we had decided this cryterion."
      ],
      "metadata": {
        "id": "BjnevFSFUp1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_bert(raw, ner):\n",
        "  sentence_id = np.arange(len(raw))\n",
        "  my_dict_1 = {}\n",
        "  my_dict_2 = {}\n",
        "  for i in sentence_id:\n",
        "    my_dict_1[i] = raw[i]\n",
        "    my_dict_2[i] = ner[i]\n",
        "\n",
        "  word_dict= []\n",
        "  ner_dict = []\n",
        "  id = []\n",
        "  for k in my_dict_1.keys():\n",
        "    for i in range(len(my_dict_1[k])):\n",
        "      id.append(k)\n",
        "      word_dict.append(my_dict_1[k][i])\n",
        "      ner_dict.append(my_dict_2[k][i])\n",
        "  s1 = pd.DataFrame(id, columns = ['sentence_id'])\n",
        "  s2 = pd.DataFrame(word_dict, columns = ['words'])\n",
        "  s3 = pd.DataFrame(ner_dict, columns = ['labels'])\n",
        "  dff = pd.concat((s1,s2,s3), axis = 1)\n",
        "  return dff\n",
        "\n",
        "df_trainbert = dataset_bert(raw_train, ner_train)\n",
        "df_testbert = dataset_bert(raw_test, ner_test)\n"
      ],
      "metadata": {
        "id": "o4rJ9F_la1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set the arguments as the batch_size, the learning_rate, the epochs and the seed. In particular the learining rate is the one that we had to tune, because it was to high and we didn't manage to make the loss decrease in other ways."
      ],
      "metadata": {
        "id": "vStyCG8lU3qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = df_trainbert[\"labels\"].unique().tolist()\n",
        "\n",
        "args=NERArgs()\n",
        "args.num_train_epochs = 15\n",
        "args.learning_rate = 0.01\n",
        "args.overwrite_output_dir = True\n",
        "args.train_batch_size = 32\n",
        "args.eval_batch_size = 32\n",
        "args.manual_seed = 46"
      ],
      "metadata": {
        "id": "Eb1UlDxfPuHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_model = NERModel('bert','bert-base-cased', labels = label, args = args)"
      ],
      "metadata": {
        "id": "_bS3mdyrP6Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train the model and verify it's performance on the test-set."
      ],
      "metadata": {
        "id": "qPf-Ox5LU_su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = BERT_model.train_model(train_data=df_trainbert, eval_data=df_testbert, acc=accuracy_score)"
      ],
      "metadata": {
        "id": "6I5GGdcFP__o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_test, model_outputs_test, pred_list_test=BERT_model.eval_model(df_testbert)\n",
        "result_train, model_outputs_train, pred_list_train=BERT_model.eval_model(df_trainbert)"
      ],
      "metadata": {
        "id": "wMLxftXKQF3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, raw_outputs = BERT_model.predict([raw_test[1], raw_test[2]], split_on_space = False)"
      ],
      "metadata": {
        "id": "FTJdGlgTusjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the results: for the training part and test part. We show the loss, the precision, the recall and the f1_score."
      ],
      "metadata": {
        "id": "KejjUxT2UXUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_train"
      ],
      "metadata": {
        "id": "jXNwyQ1eQK9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_test"
      ],
      "metadata": {
        "id": "hMHyuy7-QMTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "flat_pred_train = list(chain.from_iterable(pred_list_train))\n",
        "flat_pred_test = list(chain.from_iterable(pred_list_test))"
      ],
      "metadata": {
        "id": "BxqDrK_2OhM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_train = accuracy_score(y_true=df_trainbert['labels'],y_pred=flat_pred_train)\n",
        "accuracy_train"
      ],
      "metadata": {
        "id": "WjbveHlOOr13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test = accuracy_score(y_true=df_testbert['labels'],y_pred=flat_pred_test)\n",
        "accuracy_test"
      ],
      "metadata": {
        "id": "iQIH0eoCO1Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n"
      ],
      "metadata": {
        "id": "p1oT893ZZdAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the fact that our aim was the Named Entity Recognition, we have implemented three models: the feed forward, the BiLSTM and the BERT. The first model is the simplest one: made up of input layer, output layer and some hidden layers. The second allow also bidirectionality in order to improve the context information and the third, in addition, take advantage from transfer learning.\n",
        "If we want to interpret the results in a more precise way we should consider that the classes are unbalanced, so we should look at the wheighted avarage of the output. The results follow the expectation: a more complex model obtain better results, indeed the metrics obtained in the test set are:\n",
        "\n",
        "in the feed forward\n",
        "\n",
        "*   precision: 0.413\n",
        "*   recall: 0.369\n",
        "*   f1-score: 0.386\n",
        "\n",
        "In the BiLSTM\n",
        "\n",
        "*   precision: 0.506\n",
        "*   recall: 0.482\n",
        "*   f1-score: 0.487\n",
        "\n",
        "and in the BERT:\n",
        "\n",
        "*   precision: 0.634\n",
        "*   recall: 0.671\n",
        "*   f1-score: 0.652\n",
        "\n",
        "If we want to inspect in a more precise way, we can see that the accuracy in the test set computed through the feed forward is near to one, probably thank to the large number of \"O\" tags.\n",
        "The same can be said for the BiLSTM, for which the values are little smaller but still near to one. In addition, the accuracy for the Bert model is close to the 93%."
      ],
      "metadata": {
        "id": "NXTpbf8rPNge"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a64a2ee2"
      },
      "source": [
        "!pip install dash_core_components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69c220a2"
      },
      "source": [
        "!pip install dash_html_components"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}